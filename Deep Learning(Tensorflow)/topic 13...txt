AFTER 12

a.Applications of computer vision
b.Convolutional neural network
1. Why traditionally humans are better at image recognition than computers?
computer cannot recognize changes in how images appear
small changes affect computer prediciton
CNN can be used to combat this
ANN works well for small images
large numbers becomes quite cumbersome eg. 1920*1080*3=6mill in first layer
seconds layer say 4mill hence 24mil weights
2. Disadvantages of using traditional artificial neural network (ANN) for image classification.
3. How human brain recognizes images?
4. How computers can use filters for feature detection
5. What is convolution operation and how it works
6. Importance of ReLU activation in CNN
7. Importance of pooling operation in CNN
8. How to handle rotation and scale in CNN
https://www.deeplearningbook.org/contents/convnets.html


AFTER 13
Convolution padding and stride
. Padding allows corner pixels in image to participate well in feature detection.
 Stride is by how much amount you are moving a filter window during convolution operation.
 https://youtu.be/oDAPkZ53zKk
 

AFTER 15
Part 2  
Popular datasets for computer vision: ImageNet, Coco and Google Open images
Imagenet, Coco and google open images datasets are 3 most popular image datasets for computer vision. These datasets provides millions of hand
annotated images with classification labels, bounding boxes for object detections and image segmentation masks. Data collection is the most 
difficult part in any supervised machine learning problem and availability of such datasets makes training CNNs very easy. 

Part 3
Sliding Window Object Detection 
Sliding window object detection is a technique that allows you to detect objects in a picture. This technique is not very efficient as it is
 very compute intensive. Recently new techniques has been discovered that tried to improve performance such as R CNN, Fast R CNN, Faster R CNN
 tc. YOLO (You only look once) is a state of the art most modern technique that outperforms all other previous techniques such as sliding 
 window object detection, R CNN, Fast and Faster R CNN etc. We will cover YOLO in future videos.

 What is the window sizw? trial and error


Part 4
What is YOLO algorithm?
YOLO (You only look once) is a state of the art object detection algorithm that has become main method of detecting objects in the field of 
computer vision. Previously people used techniques such as sliding window object detection, R CNN, Fast R CNN and Faster R CNN. But after its 
nvention in 2015, YOLO has become an industry standard for object detection due to its speed and accuracy. In this video we will understand 
the theory behind how exactly YOLO algorithm works.
Only works for a single image
say person and dog...what to do?
use grid cells 
you can make all predictionsin one forward pass


AFTER 16
Part 1
RNN or Recurrent Neural Network are also known as sequence models that are used mainly in the field of natural language processing as well as
 some other areas such as speech to text translation, video activity monitoring, etc. In this video we will understand the intuition behind 
 RNN and see how RNN's work.


Part 2
 different types of RNN types such as,
1) One to many
2) Many to many
3) Many to one

Part 3
Vanishing and exploding gradients
Vanishing gradient is a commong problem encountered while training a deep  neural network with many layers. 
In case of RNN this problem is prominent as unrolling a network layer in time makes it more like a deep neural network with many layers.
  we will discuss what vanishing and exploding gradients are in artificial neural network (ANN) and in recurrent neural network (RNN)
  solutions to vanishing gradient is GRU and LSTM deals with prolem of hort memory

Part 4
Simple Explanation of LSTM
LSTM or long short term memory is a special type of RNN that solves traditional RNN's short term memory problem. I will give a very simple
 explanation of LSTM using some real life examples so that you can understand this difficult topic easily. Also refer to following blogs to 
 explore math and understand few more details.
http://colah.github.io/posts/2015-08-Understanding-LSTMs/


Part 5
Simple Explanation of GRU (Gated Recurrent Units)
Simple Explanation of GRU (Gated Recurrent Units): Similar to LSTM, Gated recurrent unit addresses short term memory problem of traditional 
RNN. It was invented in 2014 and getting more popular compared to LSTM. 

Part 6
Bidirectional RNN
Bi directional RNNs are used in NLP problems where looking at what comes in the sentence after a given word influences final outcome

Part 7
Converting words to numbers, Word Embeddings
Machine learning models don't understand words. They should be converted to numbers before they are fed to RNN or any other machine learning 
model.Techniques are,
1) Using unique numbers
2) One hot encoding
3) Word embeddings